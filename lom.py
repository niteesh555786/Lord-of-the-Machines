# -*- coding: utf-8 -*-
"""LoM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NCOyrKsBLrMljFe-pqC53cEaL_XkkbVP
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
# %matplotlib inline

from google.colab import files

uploaded = files.upload()

for fn in uploaded.keys():
  print('User uploaded file "{name}" with length {length} bytes'.format(
      name=fn, length=len(uploaded[fn])))

!ls

"""# **Importing Dataset**"""

df=pd.read_csv('df_new.csv')

df.head()

df_com=pd.read_csv('campaign.csv')

df_com.head()

subject=[]

for i in range(len(df)):
  for j in range(len(df_com)):
    if(df['campaign_id'][i]==df_com['campaign_id'][j]):
      subject.append(df_com['subject'][j])

len(subject)

len(df)

df['subject']=subject

df.to_csv('df_new1.csv',index=False)

files.download('df_new1.csv')

len(df_com)

len(df)

"""# **Trying to generate is_Open**"""

df.head()

open_df=df[['subject','is_open']]

open_df.head()

open_df['is_open'].value_counts()

from sklearn.utils import resample

# Separate majority and minority classes
df_majority = open_df[open_df.is_open==0]
df_minority = open_df[open_df.is_open==1]
 
# Downsample majority class
df_majority_downsampled = resample(df_majority, 
                                 replace=False,    # sample without replacement
                                 n_samples=12782,     # 12782 to match minority class
                                 random_state=123) # reproducible results
# Downsample majority class
df_minority_downsampled = resample(df_minority, 
                                 replace=False,    # sample without replacement
                                 n_samples=12782,     # to match minority class
                                 random_state=123) # reproducible results
 
# Combine minority class with downsampled majority class
open_dfd = pd.concat([df_majority_downsampled, df_minority_downsampled])
 
# Display new class counts
open_dfd.is_open.value_counts()

open_dfd=open_dfd.reset_index(drop=True)

open_dfd.head()

open_dfd['subject'][0]

import re
import nltk
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer

ps=PorterStemmer()

nltk.download('stopwords')

corpus=[]
for i in range(len(open_dfd)):
  osub=re.sub('[^a-zA-Z]',' ',open_dfd['subject'][i])
  osub=osub.lower()
  osub=osub.split()
  osub=[ps.stem(word) for word in osub if not word in set(stopwords.words('english'))]
  osub=' '.join(osub)
  corpus.append(osub)

len(corpus)

"""*Creating Bags of words model*"""

from sklearn.feature_extraction.text import CountVectorizer
cv=CountVectorizer()

X=cv.fit_transform(corpus).toarray()

X.shape

y=open_dfd['is_open']

#Splitting the dataset into the Training set and Test set
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)

X_test.shape

from sklearn.ensemble import RandomForestClassifier 
orf_classifier = RandomForestClassifier(n_estimators=100,random_state=12) 
orf_classifier.fit(X_train, y_train)

# Making the Confusion Matrix
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, orf_classifier.predict(X_test))
print(cm)

(1961+2298)/6391

"""# **Trying to detect relation between IS_OPEN and IS_CLICK**"""

isOpen=df['is_open']

len(isOpen)

isClick=df['is_click']

t=0
for i in range(len(isClick)):
  if isOpen[i]!=isClick[i]:
    t=t+1
print(t)

clicked=df[df['is_click']==1]
notclicked=df[df['is_click']==0]

clicked.head()

clicked_notopen=clicked[clicked['is_open']==0]
clicked_open=clicked[clicked['is_open']==1]

clicked_notopen.head()

clicked_open.head()

clicked_open['communication_type'].value_counts()

"""*The means if there is clicked then it must have been open. Logically matching !!*"""

notclicked.head()

notclicked_notopen=notclicked[notclicked['is_open']==0]
notclicked_open=notclicked[notclicked['is_open']==1]

notclicked_notopen.head()

notclicked_notopen['communication_type'].value_counts()

notclicked_open.head()

notclicked_open['communication_type'].value_counts()

"""# **Making And Merging Dataset**"""

clicked = df['is_click']
df.drop(labels=['is_click'], axis=1,inplace = True)

df_com['communication_type'][0]

com=[]
total_links=[]
no_of_internal_links=[]
no_of_images=[]
no_of_sections=[]

for i in range(len(df)):
  for j in range(len(df_com)):
    if(df['campaign_id'][i]==df_com['campaign_id'][j]):
      com.append(df_com['communication_type'][j])
      total_links.append(df_com['total_links'][j])
      no_of_internal_links.append(df_com['no_of_internal_links'][j])
      no_of_images.append(df_com['no_of_images'][j])
      no_of_sections.append(df_com['no_of_sections'][j])

df['communication_type']=com
df['total_links']=total_links
df['no_of_internal_links']=no_of_internal_links
df['no_of_images']=no_of_images
df['no_of_sections']=no_of_sections

df.insert(10,'is_click',clicked)

df.to_csv('df_new.csv',index=False)

files.download('df_new.csv')

df.head()

"""# **Dropping some columns**"""

df=df.drop(['id', 'user_id','campaign_id','send_date'], axis=1)

"""# **Visualization**"""

df.dtypes

df.is_click.value_counts()

# Overview of summary (isClicked V.S. Not Clicked)
Clicked_Summary = df.groupby('is_click')
Clicked_Summary.mean()

df.isnull().any()

df.describe()

#Correlation Matrix
corr = df.corr()
corr = (corr)
sns.heatmap(corr, 
            xticklabels=corr.columns.values,
            yticklabels=corr.columns.values)
sns.plt.title('Heatmap of Correlation Matrix')
corr

import seaborn as sns

sns.boxplot(x="is_click", y="total_links", data=df)

sns.boxplot(x="is_click", y="no_of_internal_links", data=df)

sns.boxplot(x="is_click", y="no_of_images", data=df)

sns.boxplot(x="is_click", y="no_of_sections", data=df)

#sns.pairplot(df, hue="is_click", size=3)

"""# **Down-sampling**"""

len(df[df.is_click==0])

len(df[df.is_click==1])

from sklearn.utils import resample

# Separate majority and minority classes
df_majority = df[df.is_click==0]
df_minority = df[df.is_click==1]
 
# Downsample majority class
df_majority_downsampled = resample(df_majority, 
                                 replace=False,    # sample without replacement
                                 n_samples=12782,     # to match minority class
                                 random_state=123) # reproducible results
 
# Combine minority class with downsampled majority class
dfd = pd.concat([df_majority_downsampled, df_minority])
 
# Display new class counts
dfd.is_click.value_counts()

"""# **MODELING**"""

dfd.head()

dfd=dfd.drop(['no_of_internal_links','no_of_sections','subject'], axis=1)

dfd['is_open']=dfd['is_open'].astype('object')

#dfd['is_click']=df['is_click'].astype('object')

dfd.dtypes

df_old=dfd

from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
dfd[['total_links','no_of_images']] = scaler.fit_transform(dfd[['total_links','no_of_images']])

df['is_open']=df['is_open'].astype('object')

df.dtypes

y = dfd.is_click
X = dfd.drop('is_click', axis=1)

X.head()

X=pd.get_dummies(X)

X.head()

TrainXcol=X.columns

#Splitting the dataset into the Training set and Test set
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)

"""# **Trying Various Models**

## **1.SVM**
"""

from sklearn.svm import SVC
svm_classifier = SVC(kernel='rbf', 
                #class_weight='balanced', # penalize
                probability=True)

svm_classifier.fit(X_train, y_train)

from sklearn.metrics import roc_auc_score
print(roc_auc_score(y_test, svm_classifier.predict(X_test)))

"""## 2.XGBoost"""

from xgboost import XGBClassifier
xgb_classifier = XGBClassifier(n_estimators=100)
xgb_classifier.fit(X_train, y_train)

from sklearn.metrics import roc_auc_score
print(roc_auc_score(y_test, xgb_classifier.predict(X_test)))

"""## 3.KNN"""

from sklearn.neighbors import KNeighborsClassifier 
knn_classifier = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2) 
knn_classifier.fit(X_train, y_train)

from sklearn.metrics import roc_auc_score
print(roc_auc_score(y_test, knn_classifier.predict(X_test)))

"""## 4. Random forest"""

from sklearn.ensemble import RandomForestClassifier 
rf_classifier = RandomForestClassifier(n_estimators=100,random_state=12) 
rf_classifier.fit(X_train, y_train)

from sklearn.metrics import roc_auc_score
print(roc_auc_score(y_test, rf_classifier.predict(X_test)))

"""## 5.Naive-Bayes"""

from sklearn.naive_bayes import GaussianNB 
nb_classifier = GaussianNB() 
nb_classifier.fit(X_train, y_train)

from sklearn.metrics import roc_auc_score
print(roc_auc_score(y_test, nb_classifier.predict(X_test)))

"""## 6. Decision Trees"""

from sklearn.tree import DecisionTreeClassifier 
dt_classifier = DecisionTreeClassifier(random_state=0) 
dt_classifier.fit(X_train,y_train)

from sklearn.metrics import roc_auc_score
print(roc_auc_score(y_test, dt_classifier.predict(X_test)))

"""# Predicting"""

df_test=pd.read_csv('test.csv')

len(df_test)

df_test.head()

sample=pd.read_csv('sample.csv')

sample.head()

com=[]
total_links=[]
no_of_internal_links=[]
no_of_images=[]
no_of_sections=[]
subject=[]

for i in range(len(df_test)):
  for j in range(len(df_com)):
    if(df_test['campaign_id'][i]==df_com['campaign_id'][j]):
      com.append(df_com['communication_type'][j])
      total_links.append(df_com['total_links'][j])
      no_of_internal_links.append(df_com['no_of_internal_links'][j])
      no_of_images.append(df_com['no_of_images'][j])
      no_of_sections.append(df_com['no_of_sections'][j])
      subject.append(df_com['subject'][j])

len(com)

df_test['communication_type']=com
df_test['total_links']=total_links
df_test['no_of_internal_links']=no_of_internal_links
df_test['no_of_images']=no_of_images
df_test['no_of_sections']=no_of_sections
df_test['subject']=subject

df_test['subject'][0]

"""# **Making Is_open**"""

sub=df_test['subject']

corpus=[]
for i in range(len(df_test)):
  osub=re.sub('[^a-zA-Z]',' ',df_test['subject'][i])
  osub=osub.lower()
  osub=osub.split()
  osub=[ps.stem(word) for word in osub if not word in set(stopwords.words('english'))]
  osub=' '.join(osub)
  corpus.append(osub)

X=cv.transform(corpus).toarray()

X.shape

is_open=orf_classifier.predict(X)

is_open

len(is_open)

len(df_test)

df_test['is_open']=is_open

"""# **Usual modelling**"""

df_test.head()

df_test=df_test.drop(['id', 'user_id','campaign_id','send_date','no_of_internal_links','no_of_sections'], axis=1)

df_test[['total_links','no_of_images']] = scaler.transform(df_test[['total_links','no_of_images']])

df_test=df_test.drop(['subject'], axis=1)

df_test.head()

df_test['is_open']=df_test['is_open'].astype('object')

df_test = df_test[['is_open','communication_type','total_links','no_of_images']]

X_test=df_test

X_test=pd.get_dummies(X_test)

TestXcol=X_test.columns

TrainXcol

TestXcol

for i in range(len(TrainXcol)):
  f=0
  for j in range(len(TestXcol)):
    if TrainXcol[i]==TestXcol[j]:
      f=1
  if f==0:
    print(TrainXcol[i])

Conference= np.zeros((773858,1))
Others= np.zeros((773858,1))
Webinar= np.zeros((773858,1))

X_test['communication_type_Conference']=Conference
X_test['communication_type_Others']=Others
X_test['communication_type_Webinar']=Webinar

X.head()

X_test = X_test[['total_links', 'no_of_images', 'is_open_0', 'is_open_1',
       'communication_type_Conference', 'communication_type_Corporate',
       'communication_type_Hackathon', 'communication_type_Newsletter',
       'communication_type_Others', 'communication_type_Upcoming Events',
       'communication_type_Webinar']]

X_test.head()

y_pred=rf_classifier.predict(X_test)

np.count_nonzero(y_pred)

"""# Saving file"""

#Saving the file
result=pd.read_csv('sample.csv')
result['is_click']=y_pred

result.to_csv('rf_withOpen.csv',index=False)

files.download('rf_withOpen.csv')

